[rank: 0] Seed set to 77843
WARNING:root:load from versionmodel_1_multimer_v3
wandb: Currently logged in as: abhinav2212 (gohillab). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /scratch/10110/abhinav22/output_dir/wandb/run-20240907_181319-c18tw8nu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run full_train
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gohillab/openfold_training
wandb: üöÄ View run at https://wandb.ai/gohillab/openfold_training/runs/c18tw8nu
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 77843
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/1
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]

  | Name  | Type          | Params | Mode 
------------------------------------------------
0 | model | AlphaFold     | 93.2 M | train
1 | loss  | AlphaFoldLoss | 0      | train
------------------------------------------------
93.2 M    Trainable params
0         Non-trainable params
93.2 M    Total params
372.895   Total estimated model params size (MB)
Traceback (most recent call last):
  File "/work/10110/abhinav22/ls6/openfold/train_openfold.py", line 715, in <module>
    main(args)
  File "/work/10110/abhinav22/ls6/openfold/train_openfold.py", line 464, in main
    trainer.fit(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 212, in advance
    batch, _, __ = next(data_fetcher)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 133, in __next__
    batch = super().__next__()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 60, in __next__
    batch = next(self.iterator)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/work/10110/abhinav22/ls6/openfold/openfold/data/data_modules.py", line 843, in _batch_prop_gen
    for batch in iterator:
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/work/10110/abhinav22/ls6/openfold/openfold/data/data_modules.py", line 760, in __call__
    return dict_multimap(stack_fn, prots)
  File "/work/10110/abhinav22/ls6/openfold/openfold/utils/tensor_utils.py", line 66, in dict_multimap
    new_dict[k] = dict_multimap(fn, all_v)
  File "/work/10110/abhinav22/ls6/openfold/openfold/utils/tensor_utils.py", line 68, in dict_multimap
    new_dict[k] = fn(all_v)
RuntimeError: stack expects each tensor to be equal size, but got [558, 37, 3] at entry 0 and [1734, 37, 3] at entry 1

wandb: - 0.057 MB of 0.057 MB uploadedwandb: \ 0.057 MB of 0.057 MB uploadedwandb: | 0.057 MB of 0.057 MB uploadedwandb: / 0.057 MB of 0.057 MB uploadedwandb: - 0.057 MB of 0.057 MB uploadedwandb: \ 0.053 MB of 0.092 MB uploaded (0.001 MB deduped)wandb: | 0.092 MB of 0.092 MB uploaded (0.001 MB deduped)wandb: / 0.092 MB of 0.092 MB uploaded (0.001 MB deduped)wandb: üöÄ View run full_train at: https://wandb.ai/gohillab/openfold_training/runs/c18tw8nu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/gohillab/openfold_training
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 4 other file(s)
wandb: Find logs at: /scratch/10110/abhinav22/output_dir/wandb/run-20240907_181319-c18tw8nu/logs
