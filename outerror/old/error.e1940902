[rank: 0] Seed set to 77843
WARNING:root:load from versionmodel_1_multimer_v3
wandb: Currently logged in as: abhinav2212 (gohillab). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /scratch/10110/abhinav22/output_dir/wandb/run-20240907_163423-pwjkdrcp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run full_train
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gohillab/openfold_training
wandb: üöÄ View run at https://wandb.ai/gohillab/openfold_training/runs/pwjkdrcp
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
wandb: WARNING Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Traceback (most recent call last):
  File "/work/10110/abhinav22/ls6/openfold/train_openfold.py", line 714, in <module>
    main(args)
  File "/work/10110/abhinav22/ls6/openfold/train_openfold.py", line 450, in main
    trainer = pl.Trainer(**trainer_args)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/utilities/argparse.py", line 70, in insert_env_defaults
    return fn(self, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 400, in __init__
    self._accelerator_connector = _AcceleratorConnector(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 146, in __init__
    self._set_parallel_devices_and_init_accelerator()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py", line 376, in _set_parallel_devices_and_init_accelerator
    self._devices_flag = accelerator_cls.parse_devices(self._devices_flag)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/accelerators/cuda.py", line 88, in parse_devices
    return _parse_gpu_ids(devices, include_cuda=True)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/lightning_fabric/utilities/device_parser.py", line 102, in _parse_gpu_ids
    return _sanitize_gpu_ids(gpus, include_cuda=include_cuda, include_mps=include_mps)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/lightning_fabric/utilities/device_parser.py", line 135, in _sanitize_gpu_ids
    raise MisconfigurationException(
lightning_fabric.utilities.exceptions.MisconfigurationException: You requested gpu: [0, 1, 2, 3, 4, 5]
 But your machine only has: [0, 1, 2]
wandb: - 0.057 MB of 0.057 MB uploadedwandb: \ 0.057 MB of 0.057 MB uploadedwandb: | 0.051 MB of 0.073 MB uploadedwandb: üöÄ View run full_train at: https://wandb.ai/gohillab/openfold_training/runs/pwjkdrcp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/gohillab/openfold_training
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 4 other file(s)
wandb: Find logs at: /scratch/10110/abhinav22/output_dir/wandb/run-20240907_163423-pwjkdrcp/logs
