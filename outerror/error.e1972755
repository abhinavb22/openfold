[2024-09-25 12:10:16,699] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[rank: 2] Seed set to 77843
[rank: 0] Seed set to 77843
[rank: 1] Seed set to 77843
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
[rank: 0] Seed set to 77843
initializing deepspeed distributed: GLOBAL_RANK: 0, MEMBER: 1/3
[rank: 1] Seed set to 77843
initializing deepspeed distributed: GLOBAL_RANK: 1, MEMBER: 2/3
[rank: 2] Seed set to 77843
initializing deepspeed distributed: GLOBAL_RANK: 2, MEMBER: 3/3
wandb: Currently logged in as: abhinav2212 (gohillab). Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.18.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in /scratch/10110/abhinav22/output_dir/wandb/run-20240925_121059-z5jvbom5
wandb: Run `wandb offline` to turn off syncing.
wandb: Resuming run deepspeed_training_3GPUs
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gohillab/openfold_training
wandb: üöÄ View run at https://wandb.ai/gohillab/openfold_training/runs/z5jvbom5
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:652: Checkpoint directory /scratch/10110/abhinav22/output_dir/openfold_training/z5jvbom5/checkpoints exists and is not empty.
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2]
Enabling DeepSpeed BF16. Model parameters and inputs will be cast to `bfloat16`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]

  | Name  | Type          | Params | Mode 
------------------------------------------------
0 | model | AlphaFold     | 93.2 M | train
1 | loss  | AlphaFoldLoss | 0      | train
------------------------------------------------
93.2 M    Trainable params
0         Non-trainable params
93.2 M    Total params
372.895   Total estimated model params size (MB)
Restoring states from the checkpoint path at /scratch/10110/abhinav22/output_dir/openfold_training/z5jvbom5/checkpoints/17-612.ckpt
Restored all states from the checkpoint at /scratch/10110/abhinav22/output_dir/openfold_training/z5jvbom5/checkpoints/17-612.ckpt
Using /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121/evoformer_attn/build.ninja...
Building extension module evoformer_attn...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module evoformer_attn...
Using /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121/evoformer_attn/build.ninja...
Building extension module evoformer_attn...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module evoformer_attn...
Using /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home1/10110/abhinav22/.cache/torch_extensions/py310_cu121/evoformer_attn/build.ninja...
Building extension module evoformer_attn...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module evoformer_attn...
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400410390/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400410390/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1268: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400410390/work/torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/distogram', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/experimentally_resolved', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/fape', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/plddt_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/masked_msa', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/supervised_chi', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/violation', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/tm', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/chain_center_of_mass', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/unscaled_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/lddt_ca', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/drmsd_ca', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/alignment_rmsd', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/gdt_ts', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val/gdt_ha', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/distogram_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/experimentally_resolved_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/fape_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/plddt_loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/masked_msa_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/supervised_chi_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/violation_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/tm_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/chain_center_of_mass_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/unscaled_loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/loss_epoch', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/lddt_ca', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('train/drmsd_ca', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
Traceback (most recent call last):
  File "/work/10110/abhinav22/ls6/openfold/slurm_jobs/../train_openfold.py", line 766, in <module>
    main(args)
  File "/work/10110/abhinav22/ls6/openfold/slurm_jobs/../train_openfold.py", line 506, in main
    trainer.fit(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 543, in fit
    call._call_and_handle_interrupt(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 579, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 986, in _run
    results = self._run_stage()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1030, in _run_stage
    self.fit_loop.run()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
    self.advance()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
    self.advance(data_fetcher)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
    self._optimizer_step(batch_idx, closure)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
    call._call_lightning_module_hook(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 159, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1308, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 153, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 238, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/deepspeed.py", line 139, in optimizer_step
    return deepspeed_engine.step(**kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2137, in step
    self._take_model_step(lr_kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 2043, in _take_model_step
    self.optimizer.step()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1810, in step
    self._optimizer_step(i)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 1762, in _optimizer_step
    self.optimizer.step()
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/adam.py", line 163, in step
    adam(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/adam.py", line 311, in adam
    func(params,
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/optim/adam.py", line 432, in _single_tensor_adam
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1503914) is killed by signal: Killed. 
wandb: - 0.000 MB of 0.037 MB uploadedwandb: \ 0.037 MB of 0.037 MB uploadedwandb: 
wandb: Run history:
wandb:                AlphaFoldLRScheduler ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:          train/chain_center_of_mass ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    train/chain_center_of_mass_epoch ‚ñà‚ñÜ‚ñÇ‚ñÉ‚ñÅ
wandb:                     train/distogram ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÅ‚ñá‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÅ
wandb:               train/distogram_epoch ‚ñÉ‚ñá‚ñà‚ñÅ‚ñÑ
wandb:                      train/drmsd_ca ‚ñÅ‚ñá‚ñà‚ñÖ‚ñÉ
wandb:       train/experimentally_resolved ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñÅ
wandb: train/experimentally_resolved_epoch ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÑ
wandb:                          train/fape ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÉ‚ñá‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÇ‚ñÑ‚ñÜ‚ñÇ‚ñá
wandb:                    train/fape_epoch ‚ñÅ‚ñÉ‚ñà‚ñÅ‚ñÉ
wandb:                       train/lddt_ca ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÑ
wandb:                          train/loss ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÜ
wandb:                    train/loss_epoch ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÅ
wandb:                    train/masked_msa ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÑ
wandb:              train/masked_msa_epoch ‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÅ
wandb:                    train/plddt_loss ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñà‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÜ‚ñÅ
wandb:              train/plddt_loss_epoch ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÉ
wandb:                train/supervised_chi ‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÑ‚ñá
wandb:          train/supervised_chi_epoch ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÇ
wandb:                            train/tm ‚ñÅ‚ñÅ‚ñà‚ñá‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÅ‚ñà‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÅ
wandb:                      train/tm_epoch ‚ñÖ‚ñà‚ñÇ‚ñÅ‚ñÑ
wandb:                 train/unscaled_loss ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÉ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÉ‚ñá‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÜ
wandb:           train/unscaled_loss_epoch ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÇ
wandb:                     train/violation ‚ñÅ‚ñÅ‚ñÜ‚ñà‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ
wandb:               train/violation_epoch ‚ñÅ‚ñà‚ñÖ‚ñá‚ñÉ
wandb:                 trainer/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:                  val/alignment_rmsd ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÅ
wandb:            val/chain_center_of_mass ‚ñÅ‚ñà‚ñá‚ñá‚ñá
wandb:                       val/distogram ‚ñà‚ñÑ‚ñÑ‚ñá‚ñÅ
wandb:                        val/drmsd_ca ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÅ
wandb:         val/experimentally_resolved ‚ñá‚ñÅ‚ñÅ‚ñÖ‚ñà
wandb:                            val/fape ‚ñà‚ñÉ‚ñÑ‚ñá‚ñÅ
wandb:                          val/gdt_ha ‚ñÅ‚ñà‚ñÜ‚ñÉ‚ñà
wandb:                          val/gdt_ts ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñà
wandb:                         val/lddt_ca ‚ñÇ‚ñÜ‚ñÖ‚ñÅ‚ñà
wandb:                            val/loss ‚ñà‚ñÇ‚ñÉ‚ñÜ‚ñÅ
wandb:                      val/masked_msa ‚ñà‚ñÇ‚ñÜ‚ñÅ‚ñÑ
wandb:                      val/plddt_loss ‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñà
wandb:                  val/supervised_chi ‚ñá‚ñÑ‚ñÜ‚ñà‚ñÅ
wandb:                              val/tm ‚ñÜ‚ñÖ‚ñÅ‚ñà‚ñá
wandb:                   val/unscaled_loss ‚ñà‚ñÉ‚ñÉ‚ñÜ‚ñÅ
wandb:                       val/violation ‚ñá‚ñÑ‚ñÇ‚ñà‚ñÅ
wandb: 
wandb: Run summary:
wandb:                AlphaFoldLRScheduler 0.00079
wandb:                               epoch 23
wandb:          train/chain_center_of_mass 0.0
wandb:    train/chain_center_of_mass_epoch 0.00355
wandb:                     train/distogram 2.67188
wandb:               train/distogram_epoch 1.51321
wandb:                      train/drmsd_ca 9.95485
wandb:       train/experimentally_resolved 0.12354
wandb: train/experimentally_resolved_epoch 0.05329
wandb:                          train/fape 1.37898
wandb:                    train/fape_epoch 1.28112
wandb:                       train/lddt_ca 0.57936
wandb:                          train/loss 63.75
wandb:                    train/loss_epoch 46.01103
wandb:                    train/masked_msa 0.0874
wandb:              train/masked_msa_epoch 0.10817
wandb:                    train/plddt_loss 2.98438
wandb:              train/plddt_loss_epoch 1.49127
wandb:                train/supervised_chi 0.47852
wandb:          train/supervised_chi_epoch 0.40307
wandb:                            train/tm 3.6875
wandb:                      train/tm_epoch 1.44807
wandb:                 train/unscaled_loss 3.25
wandb:           train/unscaled_loss_epoch 2.51861
wandb:                     train/violation 0.3588
wandb:               train/violation_epoch 0.11262
wandb:                 trainer/global_step 789
wandb:                  val/alignment_rmsd 16.61638
wandb:            val/chain_center_of_mass 0.02155
wandb:                       val/distogram 1.56882
wandb:                        val/drmsd_ca 10.84835
wandb:         val/experimentally_resolved 0.07237
wandb:                            val/fape 2.11106
wandb:                          val/gdt_ha 0.11713
wandb:                          val/gdt_ts 0.21295
wandb:                         val/lddt_ca 0.50759
wandb:                            val/loss 63.31985
wandb:                      val/masked_msa 0.12014
wandb:                      val/plddt_loss 1.58019
wandb:                  val/supervised_chi 0.43109
wandb:                              val/tm 1.52757
wandb:                   val/unscaled_loss 3.42165
wandb:                       val/violation 0.01735
wandb: 
wandb: üöÄ View run deepspeed_training_3GPUs at: https://wandb.ai/gohillab/openfold_training/runs/z5jvbom5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/gohillab/openfold_training
wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /scratch/10110/abhinav22/output_dir/wandb/run-20240925_121059-z5jvbom5/logs
[2024-09-25 13:30:52,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1474969 closing signal SIGTERM
[2024-09-25 13:30:52,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 1474970 closing signal SIGTERM
[2024-09-25 13:31:22,260] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1474969 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-09-25 13:31:22,563] torch.distributed.elastic.multiprocessing.api: [WARNING] Unable to shutdown process 1474970 via Signals.SIGTERM, forcefully exiting via Signals.SIGKILL
[2024-09-25 13:31:22,815] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1474968) of binary: /work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/bin/python
Traceback (most recent call last):
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.1.2', 'console_scripts', 'torchrun')())
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/work/10110/abhinav22/ls6/src/miniforge/envs/openfold_cuda12/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
../train_openfold.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-25_13:30:52
  host      : c310-004.ls6.tacc.utexas.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1474968)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: c310-004: task 0: Exited with exit code 1
